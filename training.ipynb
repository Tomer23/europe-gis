{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "eugis",
   "display_name": "eugis"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "module_path = str(pathlib.Path().absolute()).replace('/notebooks', '')\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.jobs.dataset_creation import ReadRasterFile, StridedArrayGenerator, PreProcessBorderRaster, StoreCompositeDataHDF5, PreProcessPopulationRaster\n",
    "\n",
    "raster_dem_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/dem_aggr_rst.tif'\n",
    "raster_pop_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif'\n",
    "raster_nuts_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/nuts_rst.tif'\n",
    "\n",
    "model_name = 'pop_nuts'\n",
    "\n",
    "rasters = {\n",
    "    'pop': {\n",
    "        'type': 'input',\n",
    "        'data': PreProcessPopulationRaster(ReadRasterFile(raster_pop_fn)),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'nuts': {\n",
    "        'type': 'output',\n",
    "        'data': PreProcessBorderRaster(ReadRasterFile(raster_nuts_fn), bad_value=-1),\n",
    "        'bad_value_threshold': -1\n",
    "    }\n",
    "}\n",
    "strided_generator = StridedArrayGenerator(rasters, window_size = 100)\n",
    "step = 0\n",
    "while True:\n",
    "    (train_x, train_y), (test_x, test_y) = next(strided_generator)\n",
    "    if train_x.shape[0] == 0:\n",
    "        break\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            selection_mask = [(x == i)[0] for x in train_y]\n",
    "            StoreCompositeDataHDF5(train_x[selection_mask], train_y[selection_mask], model_name + '_train_' + str(i) + '_' + str(step))\n",
    "            selection_mask = [(x == i)[0] for x in test_y]\n",
    "            StoreCompositeDataHDF5(test_x[selection_mask], test_y[selection_mask], model_name + '_test_' + str(i) + '_' + str(step))\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from scripts.jobs.dataset_creation import CreateTFDatasetFromCompositeGenerator\n",
    "from scripts.jobs.networks.conv_classifier import TrainConvClassifierModel\n",
    "\n",
    "model_name = 'pop_nuts'\n",
    "\n",
    "train_dataset = CreateTFDatasetFromCompositeGenerator(model_name + '_train', 2, batch_size = 8, window_size = 100)\n",
    "test_dataset = CreateTFDatasetFromCompositeGenerator(model_name + '_test', 2, batch_size = 8, window_size = 100)\n",
    "\n",
    "model, history = TrainConvClassifierModel(train_dataset, test_dataset, num_epochs = 10, steps_per_epoch = 1000)  # int(round(500000 / 64, 0)))\n",
    "model.save('/mnt/share/mnt/RESEARCH/SATELLITE/WORK/' + model_name + '_model') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n1000/1000 [==============================] - 224s 224ms/step - loss: 4.2102 - mae: 0.4826 - accuracy: 0.5140 - val_loss: 467376.9062 - val_mae: 0.5300 - val_accuracy: 0.4700\nEpoch 2/10\n1000/1000 [==============================] - 216s 216ms/step - loss: 13.8775 - mae: 0.2809 - accuracy: 0.7190 - val_loss: 14080.8633 - val_mae: 0.4900 - val_accuracy: 0.5100\nEpoch 3/10\n1000/1000 [==============================] - 206s 206ms/step - loss: 50.7734 - mae: 0.2181 - accuracy: 0.7820 - val_loss: 14300.3984 - val_mae: 0.5400 - val_accuracy: 0.4600\nEpoch 4/10\n1000/1000 [==============================] - 219s 219ms/step - loss: 686.8406 - mae: 0.1968 - accuracy: 0.8030 - val_loss: 92295.2891 - val_mae: 0.5200 - val_accuracy: 0.4800\nEpoch 5/10\n1000/1000 [==============================] - 225s 225ms/step - loss: 1371.9320 - mae: 0.3530 - accuracy: 0.6470 - val_loss: 104343.2578 - val_mae: 0.5000 - val_accuracy: 0.5000\nEpoch 6/10\n1000/1000 [==============================] - 243s 243ms/step - loss: 944.3262 - mae: 0.4240 - accuracy: 0.5760 - val_loss: 144285.0938 - val_mae: 0.5800 - val_accuracy: 0.4200\nEpoch 7/10\n1000/1000 [==============================] - 255s 255ms/step - loss: 197.1009 - mae: 0.4708 - accuracy: 0.5290 - val_loss: 147345.0938 - val_mae: 0.5300 - val_accuracy: 0.4700\nEpoch 8/10\n  32/1000 [..............................] - ETA: 3:58 - loss: 59.7453 - mae: 0.5625 - accuracy: 0.4375"
    }
   ],
   "source": [
    "from scripts.jobs.dataset_creation import CreateTFDatasetFromInMemoryGenerator, InMemoryStridedArrayGenerator, create_generator_sequence, SequenceSeparator\n",
    "from scripts.jobs.networks.conv_classifier import TrainConvClassifierModel\n",
    "import scripts.jobs.dataset_creation as dataset_creation\n",
    "from scripts.jobs.networks.network_factory import build_network\n",
    "import tensorflow as tf\n",
    "\n",
    "raster_ww_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/ww_aggr_rst.tif'\n",
    "raster_dem_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/dem_aggr_rst.tif'\n",
    "raster_pop_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif'\n",
    "raster_nuts_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/nuts_rst.tif'\n",
    "\n",
    "model_name = 'pop_dem_ww_nuts_linear'\n",
    "window_size = 33\n",
    "train_overfit_size = 1000\n",
    "\n",
    "rasters = {\n",
    "    'pop': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreProcessLogarithmPopulationRaster(dataset_creation.PreProcessPopulationRaster(dataset_creation.ReadRasterFile(raster_pop_fn)))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'dem': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreprocessDEMRaster(dataset_creation.ReadRasterFile(raster_dem_fn))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'ww': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreProcessWWRaster(dataset_creation.ReadRasterFile(raster_ww_fn))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'nuts': {\n",
    "        'type': 'output',\n",
    "        'data': dataset_creation.PreProcessBorderRaster(dataset_creation.ReadRasterFile(raster_nuts_fn), bad_value=-1),\n",
    "        'bad_value_threshold': -1\n",
    "    }\n",
    "}\n",
    "\n",
    "full_generator_sequence = create_generator_sequence(rasters, window_size = window_size, padding_size = 200)\n",
    "sequence_separator = SequenceSeparator(rasters, window_size = window_size)\n",
    "generator_sequences = sequence_separator(full_generator_sequence)\n",
    "train_sizes = [int(len(generator_sequence) * 0.8) for generator_sequence in generator_sequences]\n",
    "total_size = sum([len(generator_sequence)for generator_sequence in generator_sequences])\n",
    "train_overfit_size = total_size\n",
    "\n",
    "train_gen = InMemoryStridedArrayGenerator(\n",
    "    rasters,\n",
    "    window_size = window_size,\n",
    "    generator_sequences = [generator_sequence[0:int(len(generator_sequence) * 0.8 * train_overfit_size/total_size)] for generator_sequence in generator_sequences]\n",
    ")\n",
    "train_dataset = CreateTFDatasetFromInMemoryGenerator(train_gen, batch_size = 64, window_size = window_size, channel_n = len(rasters) - 1)\n",
    "test_gen = InMemoryStridedArrayGenerator(\n",
    "    rasters,\n",
    "    window_size = window_size,\n",
    "    generator_sequences = [generator_sequence[int(len(generator_sequence) * 0.8):] for generator_sequence in generator_sequences]\n",
    ")\n",
    "test_dataset = CreateTFDatasetFromInMemoryGenerator(test_gen, batch_size = 64, window_size = window_size, channel_n = len(rasters) - 1)\n",
    "\n",
    "# internal_model = build_network('simple_cnn', internal_dense_size = 100)\n",
    "# internal_model = build_network('simple_resnet', internal_dense_size = 100, input_size = window_size, channel_n = len(rasters) - 1)\n",
    "internal_model = build_network('linear')\n",
    "\n",
    "model, history = TrainConvClassifierModel(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    num_epochs = 10,\n",
    "    steps_per_epoch = 1000,\n",
    "    internal_model = internal_model\n",
    ")\n",
    "\n",
    "for i in range(0,0):\n",
    "    train_overfit_size = int(train_overfit_size * 1.2)\n",
    "    if train_overfit_size > total_size:\n",
    "        break\n",
    "    train_gen = InMemoryStridedArrayGenerator(\n",
    "        rasters,\n",
    "        window_size = window_size,\n",
    "        generator_sequences = [generator_sequence[0:int(len(generator_sequence) * 0.8 * train_overfit_size/total_size)] for generator_sequence in generator_sequences]\n",
    "    )\n",
    "    train_dataset = CreateTFDatasetFromInMemoryGenerator(train_gen, batch_size = 64, window_size = window_size, channel_n = len(rasters) - 1)\n",
    "\n",
    "    print(train_overfit_size)\n",
    "    model, history = TrainConvClassifierModel(\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        num_epochs = 10,\n",
    "        steps_per_epoch = 1000,\n",
    "        internal_model = internal_model,\n",
    "        complete_model = model\n",
    "    )\n",
    "\n",
    "model.save('/mnt/share/mnt/RESEARCH/SATELLITE/WORK/' + model_name + '_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scripts.jobs.dataset_creation as dataset_creation\n",
    "from scripts.jobs.model_prediction import PredictClassifierRaster, WriteResultRaster, FilterPredictionRaster\n",
    "\n",
    "model_name = 'pop_dem_ww_nuts_resnet'\n",
    "window_size = 101\n",
    "\n",
    "raster_ww_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/ww_aggr_rst.tif'\n",
    "raster_dem_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/dem_aggr_rst.tif'\n",
    "raster_pop_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif'\n",
    "raster_nuts_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/nuts_rst.tif'\n",
    "model = tf.keras.models.load_model('/mnt/share/mnt/RESEARCH/SATELLITE/WORK/' + model_name + '_model') \n",
    "\n",
    "a = dataset_creation.PreprocessForResnet(dataset_creation.PreProcessLogarithmPopulationRaster(dataset_creation.PreProcessPopulationRaster(dataset_creation.ReadRasterFile(raster_pop_fn))))\n",
    "b = dataset_creation.PreprocessForResnet(dataset_creation.PreprocessDEMRaster(dataset_creation.ReadRasterFile(raster_dem_fn)))\n",
    "c = dataset_creation.PreprocessForResnet(dataset_creation.PreProcessWWRaster(dataset_creation.ReadRasterFile(raster_ww_fn)))\n",
    "a = np.stack([a, b, c], axis = -1)\n",
    "# a = np.stack([a] * 3, axis = -1)\n",
    "\n",
    "data = PredictClassifierRaster(a, model, stride = window_size, channel_n = 3)\n",
    "\n",
    "out_rst_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/result_' + model_name + '.tif'\n",
    "WriteResultRaster(data, raster_dem_fn, out_rst_fn, channels=0)\n",
    "\n",
    "data = FilterPredictionRaster(dataset_creation.PreProcessBorderRaster(dataset_creation.ReadRasterFile(raster_nuts_fn), bad_value=-1), data)\n",
    "out_rst_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/result_comparison_' + model_name + '.tif'\n",
    "WriteResultRaster(data, raster_dem_fn, out_rst_fn, channels=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62c5e10acd9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtrain_dataset_generator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset_creation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInMemoryStridedArrayGeneratorForLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrasters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mlog_regr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import scripts.jobs.dataset_creation as dataset_creation\n",
    "\n",
    "raster_ww_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/ww_aggr_rst.tif'\n",
    "raster_dem_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/dem_aggr_rst.tif'\n",
    "raster_pop_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif'\n",
    "raster_nuts_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/nuts_rst.tif'\n",
    "\n",
    "model_name = 'pop_dem_ww_nuts_linear'\n",
    "window_size = 33\n",
    "training_size = 10000\n",
    "\n",
    "rasters = {\n",
    "    'pop': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreProcessLogarithmPopulationRaster(dataset_creation.PreProcessPopulationRaster(dataset_creation.ReadRasterFile(raster_pop_fn)))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'dem': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreprocessDEMRaster(dataset_creation.ReadRasterFile(raster_dem_fn))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'ww': {\n",
    "        'type': 'input',\n",
    "        'data': dataset_creation.PreprocessForResnet(dataset_creation.PreProcessWWRaster(dataset_creation.ReadRasterFile(raster_ww_fn))),\n",
    "        'bad_value_threshold': -1000\n",
    "    },\n",
    "    'nuts': {\n",
    "        'type': 'output',\n",
    "        'data': dataset_creation.PreProcessBorderRaster(dataset_creation.ReadRasterFile(raster_nuts_fn), bad_value=-1),\n",
    "        'bad_value_threshold': -1\n",
    "    }\n",
    "}\n",
    "\n",
    "full_generator_sequence = dataset_creation.create_generator_sequence(rasters, window_size = window_size)\n",
    "sequence_separator = dataset_creation.SequenceSeparator(rasters, window_size = window_size)\n",
    "generator_sequences = sequence_separator(full_generator_sequence)\n",
    "generator_sequence = generator_sequences[0] + generator_sequences[1]\n",
    "\n",
    "log_regr = LogisticRegression(warm_start=True, random_state=0, verbose = 1, solver = 'saga', max_iter = 200, n_jobs = 4)\n",
    "\n",
    "train_dataset_generator = dataset_creation.InMemoryStridedArrayGeneratorForLogisticRegression(rasters, window_size = window_size, generator_sequence = generator_sequence, batch_size = training_size)\n",
    "for i in range(5):\n",
    "    X, y = next(train_dataset_generator)\n",
    "    log_regr.fit(X, y)\n",
    "    print(log_regr.score(X[y == 0], y[y == 0]))\n",
    "    print(log_regr.score(X[y == 1], y[y == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.jobs.dataset_creation import ReadRasterFile, PreProcessBorderRaster, StoreCompositeDataHDF5, PreProcessPopulationRaster, PreprocessForResnet, PreProcessLogarithmPopulationRaster, PreProcessDEMRaster\n",
    "from matplotlib import pyplot\n",
    "\n",
    "raster_nuts_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/nuts_rst.tif'\n",
    "raster_dem_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/dem_aggr_rst.tif'\n",
    "raster_pop_fn = '/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif'\n",
    "\n",
    "pyplot.figure(figsize = (10,10))\n",
    "a = PreProcessBorderRaster(ReadRasterFile(raster_nuts_fn), bad_value = -1)\n",
    "pyplot.imshow(a, cmap='tab10')\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.figure(figsize = (10,10))\n",
    "pyplot.imshow(PreprocessForResnet(PreProcessLogarithmPopulationRaster(PreProcessPopulationRaster(ReadRasterFile(raster_pop_fn)))), cmap='viridis')  # viridis\n",
    "pyplot.show()\n",
    "pyplot.figure(figsize = (10,10))\n",
    "a = PreprocessForResnet(PreProcessDEMRaster(ReadRasterFile(raster_dem_fn)))\n",
    "pyplot.imshow(a, cmap='viridis')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = ReadRasterFile('/mnt/share/mnt/RESEARCH/SATELLITE/WORK/pop_rst.tif')\n",
    "a[a < -10] = -10\n",
    "a[(a < 3) & (a > -10)] = 0\n",
    "a[a > 2] = 1\n",
    "pyplot.hist(a, bins='auto') \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}